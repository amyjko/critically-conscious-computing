Whereas humanities, social sciences, and the arts are primarily about understanding and celebrating human experience, science is about understanding the natural world, as it is and as it changes. Whether studying the physical world in physics or chemistry, the living world in biology or genetics, or the broader world in earth and space sciences, our constant pursuit of better theories and models of our material world has profoundly changed humanity. Science has been the foundation of nearly every technology innovation, every engineering advancement of our built environments, and every new vaccine, therapy, or cure for viruses and diseases in the past century. While it is not at the center of _every_ advance in our quality of life, it is at the center of many of them, and for that reason alone, it is a fundamental literacy.

Efforts to define this literacy through the [Next Generation Science Standards|https://www.nextgenscience.org] (NGSS) have offered many innovative learning progressions and ideas to science education. The standards move well past rote memorization of scientific formulas to engaging youth in understanding scientific practices such as posing questions, using models, planning investigations, analyzing data, using mathematics, and developing explanations. These many core practices are coupled with cross-cutting concepts such as cause and effect, patterns, structure and function, and systems. We’re only at the beginning of understanding the implications of these standards on science learning, but it’s clear that they have the potential to reshape science literacy in profound ways, at least in the United States.

In some ways, computing literacy is intrinsically misaligned with science literacy. Computing is often about the _artificial_, about inventing new futures, about reasoning formally about abstract symbols, and about engineering practical software systems to support communication, creativity, and connection. Science, in contrast, is about the _natural_: the physical laws that make up reality, the biological laws that govern life, and the dynamic systems that sustain and sometimes threaten life. It can therefore be hard to see how computing might have anything to contribute to science, aside from simply being one of many tools for doing science.

Up until the invention of the computer, this view would have been accurate. After all, science progressed rapidly for hundreds of years without digital computers and likely could continue to do so. And yet, the advent of the digital computer has had profound effects on science at every level. Personal computers have become a central part of every scientific laboratory. Supercomputers have become essential tools in precisely modeling our planet and its climate. Mobile computers have become essential field devices for scientific field work and collaboration. And embedded computers, especially sensors, have become essential instruments for measuring everything from our atmosphere to the physiological processes of our bodies. Talking about science without talking about computing would misrepresent the inherently computational nature of modern science. And NGSS reflects this, prominently featuring computing as a core scientific practice alongside mathematics.

In the rest of this chapter, we’ll discuss the many ways that computing is now woven through science. We’ll then review the many opportunities to integrate computing into science and give one example of a unit that does so critically.

|question.jpg|TODO|Code is now a scientific instrument.|@ashley|

# Intersections

Computing plays two major roles in science: as an artifact that enables science and an artifact enabled by science. These two mutually beneficial roles occur across the sciences. We’ll discuss them across the three major NGSS divisions of natural science: physical, life, and earth and space.

## Physical sciences

The physical sciences -- _physics_ and _chemistry_ -- examine the foundational elements of our natural world: force, reactions, energy, matter. These properties of our universe can be fascinating because they are so far reaching, affecting every natural system in surprisingly predictable ways. But they are also fascinating because they are so often invisible to human perception: we cannot see force, we cannot see reactions, and we cannot see energy, at least with our natural senses of sight and hearing.

|Chapter23_Image_Particle_Accelerator.jpg|A bright orange glowing cylinder, the center of a particle accelerator.|A particle accelerator, which is also a massive computer.|CERN|<

Computing has become indispensable in making these natural phenomena visible. For example, simple instruments to measure force are now predominantly digital, helping us precisely measure the product of mass and acceleration. More complex instruments such as particle accelerators, which are massive physical and computational laboratories, let us observe even the weakest of forces at the finest levels of precision. Algorithms, and applications built around them, help us computationally model the molecular structure of everything from simple compounds to complex proteins, enabling us to discover new genetic markers and develop new medicines. All of these forms of computational measurement have enabled the physical sciences to gather ever more precise data on ever smaller phenomena and more reliably and repeatedly analyze this data than ever before.

|Chapter23_Image_Chip.jpg|A greyscale network grid of lines on a microprocessor.| A complex grid of 5 nanometer copper wires in a microprocessor, enabled by physics and chemistry.|NISE|

Of course, physical sciences have also enabled computing. Computer hardware, for example, are interdisciplinary engineered artifacts requiring knowledge of electromagnetism, materials sciences, chemical reactions, and with the slow advent of quantum computing, even subatomic forces. Even the humble transistor was a pinnacle of applied science, building upon a century of discoveries about voltage, current, insulators, conductors, and semiconductors. And our most advanced understanding of physics and chemistry will be the sciences that enable entirely new kinds of computational devices, ever smaller and more computationally powerful. 

And as computing has matured through science, it has returned dividends to scientific discovery. For example, the particle accelerators that have enabled us to examine the behavior of subatomic particles are surprisingly reliant on software to not only capture and store data, but to analyze it at unprecedented levels of precision. The experiments run on in particular accelerators, for instance, require highly precise numerical calculations, far more precise than can be done with a consumer computing device. The flexibility of software has enabled physicists to write custom FORTRAN APIs that perform arithmetic and extremely high levels of numerical precision, giving exceptionally high confidence about the behavior of exceptionally small particles. None of this science would be feasible if scientists were still doing calculations manually.

## Life sciences

For much of the history of the life sciences, discovery was about description and categorization of new species and new ecosystems. This began to change with two major advances: the theory of evolution, which posited that species change through successive generations of reproduction and through the discovery of DNA in the late 19th century. Since those theories emerged and the structure and function of DNA and its role in evolution was confirmed, biology has rapidly shifted from a science of classification to a science of modeling and prediction, largely governed by the central role of DNA and RNA in encoding life.

This is where computing comes in. The profound discovery that all species are encoded as largely similar but critically and subtly different sequences of A, G, C, and T nucleobases draws direct parallels to the sequences of 1’s and 0’s from which software is composed. Of course, these parallels are shallow — DNA encodes infinitely complex, recursive, and often noisy interactions of protein folding, while binary encodes well-defined, fully observable logical structures such as encoded data and machine instructions —and so the metaphor is not particularly constructive. However, the shared role of encoding in life and computing does mean that computers are highly suitable to digitally analyze genetic data.

|Chapter23_Image_COVID19.jpg|A radial visualization of the relationships between SARS-CoV-2 viruses found globally.|Understanding how COVID evolves requires computational analysis of  RNA sequences found globally.|Unknown|

For example, the field of ~bioinformatics~bioinformatics emerged as soon as scientists were able to extract and digitally store sequences of DNA for analysis, along with other biological structures such as nucleotide and amino acid sequences, protein domains, and protein structures. Computing helps organize this data, make it available for retrieval and analysis, but also are foundational to analyzing them. Human cells, for instance, generally have DNA sequences that are approximately 3 _billion_ base pairs long. Without computers, there would simply be no way to process or analyze that much data. And in fact, that much data is even hard for computers to process, as DNA isn’t simply a sequence of instructions like computer programs are, but rather encodings that influence how other encodings are interpreted. Bioinformatics is therefore key to analyzing these many complex interactions, and algorithms and data structures are the central tool of bioinformatics.

|Chapter23_Image_Bobcat.jpg|A black and white night shot of a bobcat staring down a camera.|This bobcat found the motion sensitive camera.|AZCentral|

Computing has also enabled new types of observations of life. Large cats, for example, are notoriously difficult to observe in the wild as they generally avoid -- and can easily detect -- the presence of people. This makes understanding the nature of their broad impact on ecosystems difficult to study. But tiny cameras, with software that can analyze a scene in real time to detect movement and begin recording, have enabled life scientists to quietly observe the habitats and behaviors of large cats and countless other species, revealing fascinating forms of animal intelligence. Similarly, remotely controlled high-flying drones equipped with cameras have allowed systems-level views of ecosystem dynamics such as hunting and migration, often at much greater levels of fidelity than trapping and tagging small samples of populations. Programmable devices, configured by scientists, are therefore giving us ever greater visibility into the formerly invisible parts of nature.

|Chapter23_Image_FoldIt.png|A 3D model of a protein sequence with toolbars and scores.|Fold.it is a game that enables people to identify what shape proteins will take by interactively navigating the constraints imposed by the particular structure and forces of a given protein.|Fold.It|

Computing has also enabled new forms of public participation in life sciences. For example, games like [Fold.It|http://fold.it] have engaged hundreds of thousands of people in solving the hard problem of protein folding, which is the process by which complex protein sequences encoded in DNA are eventually “folded” into three-dimensional structures. This game invites citizens to search for how proteins are folded while following the constraints imposed by physics and chemistry. The result has been that many untrained scientists, scaffolded by a carefully designed game, have made many discoveries about the structure of proteins, enabling new insights into diseases and how to treat them. Other citizen science projects like [OceanEyes|https://www.citizenscience.gov/catalog/511/#] engage the public in helping to analyze the millions of images of undersea life, helping to model fish life and its relationship to the broader ocean ecosystems. Computing makes this possible by sharing large amounts of data with hundreds of thousands of volunteers to tag and locate fish in images.

## Earth and space sciences

The core topics in Earth and space sciences span the universe and its stars, Earth and the solar system, and the history of planet Earth. These ideas are coupled with other central concepts fundamental to other sciences, including how matter formed during the Big Bang and within stars, how the behavior of the sun changes our lives. These many ideas, each intersecting with the NGSS concepts of patterns, scale, proprition, quantity, energy, matter, and stability, are pervasive, helping to model and explain the mechanics of the universe and our planet’s position in it.

Computing has, in essence, been a tool for advancing Earth and Space sciences. It has been core to launching spaceships and satellites beyond Earth’s gravitational pull, helping us take ever more detailed imagery and measurements of nearby planets and distant galaxies. It has helped us analyze this data, giving us ever more precise insights into the composition of stars and planets, of the behavior of gravity, and of the origins of the universe. And computing has also enabled us to better understand and share these discoveries, sharing images of the universe and making immersive simulations of space that help us comprehend its sheer scale. Every one of these innovations have required data, code, and algorithms that intricately model ever evolving physical, chemical, and biological laws through logic.

|Chapter23_Image_Hypoxic.jpg|A map of the Gulf of Mexico showing a long band of red and orange with low oxygen levels.|A massive installation of sensors sends data to a central database, giving a near real-time view into the oxygen levels in the Gulf of Mexico.|EPA|<

One example of the role of computing is the consortium of U.S. National Centers for Environmental Information. These centers are some of the most elaborate Earth monitoring and large scale computing centers on the planet. One of these centers is the [Gulf of Mexico Hypoxia Watch|https://www.ncei.noaa.gov/products/gulf-mexico-hypoxia-watch], which has installed an elaborate array of oxygen sensors at the bottom of the Gulf of Mexico. Oxygen is key to sustaining the gulf’s healthy population of shrimp, fish, and other marine life. When oxygen in water is too low, marine life dies; runoff from the local oil and agricultural industry into the gulf often causes this hypoxia, threatening the local ecology and the people who depend on it. Computing is central to all of this: the sensors digitize oxygen levels in real time, bundle the data and transmit over the internet, store it in a database, which is queried by thousands of scientists in government, academia, and industry to monitor the gulf, make predictions, and generate alerts. This project is just one of hundreds of computing-enabled projects that give us a detailed and predictive understanding of the state of our planet and its likely impacts on life on Earth.

Computing has also enabled Earth and space sciences to engage the public in data analysis through what is now called ~citizen science~citizenscience. One of the first examples of this was [SETI@Home|https://setiathome.berkeley.edu], a project launched at the University of California, Berkeley, to monitor real-time streams of radio transmissions emitted from space for several kinds of signals of potential extraterrestrial life. Signals included things like spikes in transmission power, pulsing signals, and repeating waveforms. Algorithms powered all of this signal detection, taking digital forms of radio transmission as input, analyzing the characteristics of the radio frequencies for these signals, and then flagging any segment as anonymous for human review. There was just one problem with this approach: the huge amounts of radio transmission data. SETI\@Home solved this by allowing the public to install software on their computers, distributing analysis across hundreds of thousands of personal computers. Users did not need any particular scientific knowledge to participate; they just needed to be willing to give up some of their computing power and electricity to help with analysis.

|Chapter23_Image_Rover.jpg|A wheeled robot on the surface of mars interacting with a rock.|We sent this robot to another planet and remotely controlled it for 14 years to analyze the planet’s composition.|NASA|<

Computing has also been central to space exploration. For example, NASA has now sent several robots to the nearby planet Mars to support soil analysis, explore craters, analyze the atmosphere, and take pictures of the planet. _How did we get a robot to Mars?_ Engineering was central, of course, as robotics is partly a discipline of mechanical, electrical, and industrial engineering. But robots also require software: computing, and many decades of research on machine learning and robotic intelligence, enabled the rover to safely land, to navigate rocky terrain, to self-maintain its lenses and equipment, to recharge its own batteries, and to monitor environmental conditions for threats to its safety. But computing also enabled the Mars Rover team to communicate with the robot. Each day (roughly 25 hours Earth time), the Mars Rover team would write and debug computer programs for the rover to follow to  achieve a particular scientific mission, transmit the program 200 million miles away to the Rover on Mars, and then wait for it to execute and return results. Debugging a computer program on the computer in front of you is hard, but at least you can watch the program execute. Debugging a program that takes 18 minutes to transmit to a robot on another planet is a different challenge entirely.

## Science, computing, and bias

While the impact of computing on science has been profound, rapidly accelerating advances and opening up previously invisible worlds of space, ocean, and subatomic particles, science has been subject to some of the same oversights as mathematics, statistics, and computing. For example, a foundational problem is one of sampling: what life is studied, what parts of the Earth are studied, and who in society is studied? These choices by scientists directly shape the knowledge we have about our bodies and our world in ways that can advantage some and disadvantage others. For example, for centuries, health scientists have predominantly studied white men, often assuming that insights about this narrow demographic group would generalize to women and other ethnic groups and cultures. Research has clearly shown that they do not: biological diversity, ironically, applies to humans too, and so many of our research findings about human anatomy and physiology are unfortunately shaped by our gender and racial hierarchies. Computing, in accelerating the science of biology, has only further masked this bias, shielding it behind a narrative of algorithm and data objectivity.

Similar kinds of bias exist in sciences concerned with nature more broadly. For example, there is unequal access to participating in science: those with computers and access to the internet can participate in citizen science, learn of scientific discoveries, and connect with scientists in ways that those without cannot. Because these disparities persist along class and racial boundaries, science itself is composed of predominantly upper class white people, who are driven by curiosities shaped by their perspectives and concerns. If science more broadly reflected humanity, and the computational tools of science were more equitably distributed to humanity, what scientific questions would we be asking that we are not asking now? And how might the answers to those questions lead to different, and perhaps more equitable social progress?

Many of these issues have become more visible and consequential with the advent of ~data science~datascience — a phrase coined in industry to refer broadly to the use of algorithms to answer questions with data —and has since been adopted in academia to refer to the use of algorithms and large scale data in science. As we discussed in [CS + Mathematics|math], data science is closely tied to the use of algorithms and statistics to analyze data. In science, however, data science techniques have been key to enabling science to harness large data sets. But in the process, many of the issues above about representation, measurement bias, and even what questions are asked, have become amplified. Whereas science was once making biased conclusions on small data sets slowly, it is now making biased conclusions on large data sets quickly, excluding people at the margins at greater rates than ever before.

# Teaching science with computing

As we have discussed above, science has always engaged data as a central part of discovery. But the advent of computers, just as other scientific instruments have throughout history, has transformed science into a computational discipline, using computing to gather data, analyze it, share it and communicate it. Science education has responded to this shift in science, partly through the Next Generation Science Standards, but also culturally, [recognizing|https://kappanonline.org/math-importance-emergence-k12-data-science-lamar-boaler/] that the use of data and computing through data science is now central to science literacy.

Research has responded to these shifts by examining how science education might change to incorporate computing authentically. Some of the earliest explorations into integrating science and computing came in the 1990’s, with diSessa’s applications of Boxer<disessa86> to science education<sherin06>, which was one of the first explorations into using programmable, computer-based simulations of concepts such as gravity and force. Since Boxer, explorations of computer-based simulations in science have evolved, including pedagogy in which students create their own models of natural phenomena with code<damelin17> and using computational simulations to not only examine concepts in skills, but also students’ local communities, policies, and politics as they intersect with the natural world<freidenfelds20>. Many of these explorations engage transdisciplinary approaches to science education, combining ecology and computation<panorkou20,gunckel20,fick20>, art, computing, and science<finch21>, and mathematics, computation, and science <basu20>. These research explorations have evolved into whole collections of curricula and activities, such as [Luminous Science|https://www.playfulcomputation.group/luminous-science-curriculum-home.html], [Project GUTS|https://www.projectguts.org], and [CT-STEM|https://ct-stem.northwestern.edu], all of which carefully engage computation in service of science literacy.

One benefit of simulations is the way they might engage learners. For example, some students have demonstrated that the use of computational simulations is strongly associated with student sustained engagement in science learning in class, and that such engagement promotes learning<inkinen20,radke20>. Such work has found that these strong associations are likely causal: by engaging students in actively manipulating simulations and freely forming and testing hypotheses in line with their curiosities, students are far more likely to attend to science concepts than if passively engaging them through textbooks or lectures, or even through laboratory experiments, which are often necessarily rigid in their design.

Another well-researched potential benefit is how simulations can shift learners’ conceptions of natural phenomena from more brittle situational models of natural behavior to more principle-based interpretations and predictions. For example, in work investigating students using computational simulations of earth, students quickly move from more literal interpretations of simulation behavior to more general principles of the behavior of climate and Earth’s atmosphere<basu19,covitt20>. Other investigations into the intersection of climate change through mathematics and computation have found that students are able to shift from mathematical models, expressed as code, into higher-level conceptual reasoning about climate change<basu19b,basu20b>. Engaging students in building models by writing code that simulate groundwater contamination<gunkel20b>, molecular diffusion<wilkersonjerde15>, force<chang20>, and other scientific phenomena<wiese21> have shown that the act of constructing models of natural phenomena with code can help students develop deeper conceptual models of these phenomena, by engaging them in explicitly and interactively trying to construct lower-level models that reflect their conceptual understanding.

These benefits do not come without costs and risks. For example, when students engage representations of models as code, they inevitably end up testing, evaluating, and debugging models just as with any other form of programming<eidin20>. This can sometimes cause students who are struggling with things such as language syntax to focus more on surface features of simulations rather than deeper explanatory mechanisms<grooms21>. And when the code used to build models is notationally distant from science and math notations, students often struggle to translate between these multiple representations, creating more work for teachers to scaffold translation and limiting transfer from code representations to science literacy<rafalski19,hutchins21>.

While most research has explored the use and construction of simulations, placing science at the center and computing at the periphery, some scholars have examined the opposite, centering data and computation in science education. Several projects, for example, have focused on data cleaning and manipulation as a way to bring computing to science education, engaging students in writing code to clean messy science data sets<petersburton20>, structuring public datasets for analysis<wilkerson21>, and analyzing data with code<schanzer22>. This approach, which places data and computation at the center of students’ attention, and science at the periphery, has evolved into mature curricula such as [Bootstrap Physics|https://bootstrapworld.org/materials/physics/], which uses code to help students build computational and mathematical models of the physical world.

Other scholars have examined ways of using computing in science education to make science education more critical and culturally responsive. For example, some have called for more student-centered choices in data and data analysis, examining personal, cultural, and political data sets<lee21>. Many have engaged these more culturally and critically situated approaches to science education, including through local habitat restoration<birney19>, jobs<birney21>, social impacts of watershed degradation<caplan21>, intersections between chemistry and Black culture and community<lachney21c>, and more. These perspectives on science literacy critically examine society through a scientific lens and use computation as an expressive and analytical medium for this examination.

An additional approach has been to use computing as a medium for scientific communication. For example, some scholars have found that having youth report the results of scientific experiments through computational storytelling can engage youth in the personal meaning of scientific concepts and practices<wilkerson18,biddy20,lanouette20>. Others have layered on collaboration into storytelling, finding that youth can develop not only greater cultural competency of their peers, but also deeper science literacy through engaging their peers’ unique conceptions of scientific concepts<mott19,wilkerson20>.

Teachers across different scientific disciplines generally find all of the approaches above resonant, appreciating the active learning required to manipulate and construct simulations, data, and narratives<corum19,birney19b,birney19c>. However, many efforts to engage teachers in professional development on computational science education raise a recurring question: what is the computation for<wilkerson16>? The more that teacher education can concretely demonstrate how computing can achieve all of the learning benefits above, the more teachers tend to embrace computation as a means to science literacy and see it as an authentic part of science.

## Unit sketch: Data, science, and exclusion

Many of the pedagogical explorations described in the previous section focus on developing a kind of optimistic scientific literacy, one that celebrates the beauty and wonder of science, and connects science to students’ individual lives. But another key part of scientific literacy is understanding its limits: what does it struggle to investigate, how does it manage bias, and what role does interpretation play in shaping the scientific truths that we arrive at in science? Data is a critical part of all of these limits, and with the advent of data science, applying computing to gathering and analyzing data, understanding the role of data in science is key.

The unit sketch below tries to achieve this by engaging students in gathering, structuring, and analyzing data scientifically, but also personally, having students gather data about themselves. Through a series of data gathering exercises, students examine how data is central to science, and how data’s limits shape the limits of science.

After this unit, students will be able to:

* Answer basic statistical questions using a data set and scripting
* Critically reflection the limitations of the data and scripting for answering questions 

=
### Session 1: Spreadsheets as a laboratory
* Present spreadsheets as an authentic tool of science, for storing and analyzing data.
* Share a link to a shared spreadsheet (e.g., Google Sheets) and make sure every student has editing permissions to the spreadsheet.
* Illustrate the use of spreadsheets for science by beginning with an example question: what is the distribution of heights in the classroom? Prompt every student to write their name in the first cell in a row, then write their height. (Don’t specify the unit — having variations in formatting creates a learning opportunity).
* Propose to compute the minimum height of all of the height’s entered. Ask the students how they might do that, soliciting problems that might need to be fixed. (This will likely surface problems of inconsistent units and formatting and proposals to scan for the smallest number). 
* Work through the problems they identify, prompting students to fix formatting and introducing the concept of formulas that refer to cells and cell ranges.
* Next, propose to compute the average. Prompt students for their guesses about what the average might be, then compute the average. What were their guesses based on? How close were they?
* After discussing the average, prompt students to discuss with their neighbor: do they trust the number? What might be wrong about it? If the data is wrong, what does that mean about the average?
* After discussing the average, prompt students to discuss with their neighbor: how does average make them feel? What does the average mean to them?
* Return as a group and deconstruct what “normal” means. Is there a normal height? What is the average height useful for? What might that number be used for?
*  Allude to future sessions, in which they will examine more complex personal data.
=

This first session does a lot of work, introducing spreadsheets as a data analysis platform, going through a full data science pipeline of gathering, analyzing, and interpreting data, and also surfacing critical questions about the analysis, including bias and errors in the data, bias in the aggregate statistics computed, and bias in how the statistics might be used. The next session builds on this, examining a more problematic personal data set.

=
### Session 2: Distance to school
* Pose another question: what is the typical distance that students live from school? Prompt students to share critiques of the question. What kind of data is it asking for? What does distance mean? Converge toward an agreed upon measure of distance and a unit.
* After deciding on a measure of distance, prompt students to compute their distance using mapping software and share a new spreadsheet in which they can enter the data. Encourage students to help each other with their measurements.
* Prompt students: do they trust the distance measurements they computed? On what other data was that based? How might they verify their distance?
* Compute the average distance, then have students discuss with their neighbors: does the average capture their commuting experience? Return as a group and surface the ways that it does and does not reflect their commute.
* Propose to capture a different measurement: time to commute. Engage in a discussion about how to measure it: should it be based on students’ self-report, a more precise measurement, or a prediction from mapping software?
* Have students determine their commuting time from mapping software and enter it in a new column in their row of the spreadsheet. Compute the average commuting time and once again have them discuss with their neighbors: does commuting time capture their commuting experience? Return as a group and discuss the problems with the measure.
* Returning to the final question of the first session, ask the students what “normal” means. What if the average commuting time was used to determine when school starts?
=

This second session offers some repetition of the first, but builds on it by introducing a more complex data collection process, with more fraught measurement bias and applications. It deepens the critical examination of the limits of averages and makes salient any student -- most students -- who do not cluster around the center of the distribution. The next session builds upon this by examining an even more problematic metric.

=
### Session 3: Pollution
* Propose to investigate differences in air quality, discussing the various risks of higher pollution and the various causes (e.g., emissions from fuel-based cars, manufacturing). Since students are likely to live in the same region with similar air quality, have them investigate a country that they feel connected to somehow (e.g., where they might have relatives or where they might have visited, defaulting to their country of residence if they don’t have one). Show an air quality map and debate how they might measure air quality (current, historical, average over the past year, etc.). 
* Share a new spreadsheet for gathering data and have each student enter two columns in the spreadsheet: a country in one column and their air quality measurement in the next column.
* *Formative assessment*. Have students form small groups: do they trust the pollution they obtained? What is their source? What makes the data credible? Do they trust their own measurements based on that data? Have them generate a list of at least five observations about the data. Return as a group and have each group present the potential risks of the data in whatever form they choose.
** This is _responsive_ because it centers students’ personal judgements about how the data might misrepresent their communities.
** This is _participatory_ because it has students shape the format for their report outs.
** This is _educative_ because it helps students think critically about data sources and communicate those criticisms.
* Visualize the data with a histogram, making the distribution visible. Identify the outliers on the chart and call on the individual students that entered them, having them tell the story of why they chose the country that they did, how they chose the number that they chose, and what that number might mean to them.
* As a group, discuss what shape they think the distribution should have. Should all of the numbers be lower? Should it be narrower? Is it fine the way it is? Reflect with the group on how this data might be used to make air quality policy.
=

This third session adds a number that is more collective, but potentially still personal, engaging students in examining the relationship between the atmosphere and the health and wellbeing of their families. It also introduces distributions as a way of reasoning about a sample of data, while still making salient averages and outliers. This builds toward the final sessions of the unit, which engage students in constructing their own data analysis projects.

=
### Session 4: Make it personal
* Have students ponder their own question: what do they want to know about their communities, their classmates, or the natural world around them? Have them form small groups to brainstorm, and have them find a question of interest that tries to describe a distribution of some measure. Have them research what data might be publicly available or easily obtained from classmates to help them answer it.
* Return as a group and then have students share what they brainstormed, capturing each question in a spreadsheet and putting their names down in a cell. Ask if any students would want to join a question as a team rather than working on the question they brainstormed. Produce a final list of questions and team assignments.
* *Summative assessment*. Charge each team to then go develop a simple data collection and analysis plan like the ones presented in the previous sessions and prepare to represent for 2-3 minutes in a format like an academic conference, where students are presenting their discoveries but also receiving critical, skeptical questions about them. What exactly is their question, how exactly will they measure it, and how will they analyze the data they find or measure? How do they want to be evaluated on their presentations?
** This is _responsive_ because it centers students’ own curiosities.
** This is _participatory_ because it encourages students to reflect on what makes for a sound scientific investigation.
** This is _educative_ because it engages students in authentic practices of attending to soundness, both in planning, executing, and communicating their research.
* Before the end of the session, have students present their plans, soliciting feedback from classmates, and offering feedback about potential refinements of their plan. Focus critiques on potential sources of error and bias.
=

This session builds upon the scaffolding of the first three sessions to challenge students to identify their own questions of personal meaning, while also providing opportunities for them to see how others perceive their question. This sets up the next session, which focuses on data gathering and analysis.

=
### Session 5: Gather and analyze data
* Have students regroup as teams and refine their plan based on the previous sessions feedback.
* Prompt them execute their plan, gathering data from the internet or from their classmates, entering it into a shared spreadsheet, and being mindful of measurement data entry errors.
* Support students in their data analysis, helping them fix syntax errors, identify formulas to use, and verify the semantics of their data analysis formulas. Encourage them to support each other if they notice peers who get stuck.
* As the session nears its end, prompt teams to present their discoveries in a 5-slide presentation, covering 1) what their question was and why they wanted to answer it, 2) what data they gathered to answer it and how, 3) what analysis they did to answer their question, 4) what they discovered, 5) what limitations their discovery has that might threaten the validity of their answer.
=

This session offers students open time to conduct their scientific research with agency and curiosity, while getting support from peers and the teacher. It also primes them to communicate their results, scaffolding the structure of their presentation to ensure that it covers the key elements of scientific communication.

=
### Session 6: Present and critique
* Remind students of the format of the presentations and of the co-constructed presentation rubric. Each group presents for 2-3 minutes, then gets 1 or 2 questions from their classmates.
* Have students present and answer questions; after each presentation, solicit class feedback in relation to the co-constructed rubric.
* After the presentations, return to some of the themes from the unit: 1) the often invisible problems with data, 2) the subjectivity of deciding how to measure things, 3) the risks and benefits of using code to analyze data, and 4) the many potential pitfalls of overinterpreting scientific results and using them to make policy decisions.
=

By focusing on a data science pipeline, this unit sketch makes room for many critical questions about data, computation, and data science, while also immersing students in many authentic practices in science. It also strives to center students themselves, not only in practicing science, but in being measured and in measuring things that are of meaning to them, showing them that science is a collection of tools, practice, and methods that can be applied to many things, including the many things in their lives.

But the unit brevity also means that it does not go deep on any particular scientific topic. It might make more sense in a general science class in middle school, for example, where there is less curricular pressure to focus on a particular area of science. Or, it could be adapted for a particular subject, focusing on questions related to physics, life, or Earth sciences, at the expense of some freedom in what phenomena students choose to examine. A longer unit might help achieve breadth and depth, giving a broader introduction to scientific practices, but extending the project in much more depth, helping students examine measurements and analysis in much more detail.

# Conclusion: seeing nature through digital eyes

Ultimately, teaching science -- including within the bounds of the Next Generation Science Standards -- does not require teaching computer science. And this is reasonable, as science can be done, in principle, with computers. But doing so would ignore an important reality in the broader global enterprise of science: data and algorithms are an inescapable, integral part of modern science and so ignoring them in science classrooms would present a false reality.

But integrating computing into science isn’t just about authenticity. Integrating computing into science education can be about wonder, illustrating the ways that computing has joined forces with science to enable us to see other galaxies, to visit other planets, to watch, in real time, how our oceans and the air we breathe change, affecting our food, our health, and our wellbeing. Integrating computing into science education can also be about justice, examining the ways in which science and computing can reveal inequities in the world, but also how it can mask them. These intersections are not just nice to have, but essential in a world in which science and computing are moving faster than ever and shaping society in ever more unpredictable ways.

@standardsHeader
@standardsBlurb

@cstaHeader
@csta2DA09
@csta3ADA10
@csta3ADA11
@csta3BDA05
@csta3BDA06
@csta3BDA07

@toleranceHeader
@tolerance9
@tolerance12

@teacherHeader
@teacher1d
@teacher2c
@teacher4b
@teacher4d
@teacher4e
@teacher4f
@teacher4g